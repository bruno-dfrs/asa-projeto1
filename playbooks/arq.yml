---
- name: Apply exclusive configurations to the "arq" machine
  hosts: arq
  become: true

  vars:
    partitions: # Additional disk partitions to be created (from Vagrantfile disks)
      - sdb
      - sdc
      - sdd
    vg_name: dados # Name of the LVM Volume Group
    lv_name: ifpb # Name of the LVM Logical Volume
    lv_size: 15G # Size of the Logical Volume
    mount_point: /dados # Mount point for the Logical Volume

    nfs_user: nfs-ifpb # System user for NFS ownership and squashing
    shared_dir: /dados/nfs # Directory to be exported via NFS
    subnet: 192.168.56.0/24 # Network allowed to access the NFS share

  tasks:
    ### ──────────────────────────────────────────────────────────────────
    ###                               DHCP
    ### ──────────────────────────────────────────────────────────────────

    # 1. Install the ISC DHCP Server package
    # This provides the Dynamic Host Configuration Protocol service to assign IPs on the private network.
    - name: Install ISC DHCP Server
      ansible.builtin.apt:
        name: isc-dhcp-server
        state: latest

    # 2. Deploy the DHCP server configuration file
    # Copies a pre-configured dhcpd.conf file that defines the IP pool, lease times, and options for the network.
    - name: Deploy DHCP server configuration
      ansible.builtin.copy:
        src: ../files/dhcp/dhcpd.conf
        dest: /etc/dhcp/dhcpd.conf
      notify: restart isc-dhcp-server

    # 3. Set the network interface for DHCP service
    # Configures the DHCP server to listen and serve requests on the private network interface (eth1).
    - name: Configure DHCP server interface
      ansible.builtin.lineinfile:
        path: /etc/default/isc-dhcp-server
        regexp: "^INTERFACESv4="
        line: "INTERFACESv4='eth1'"
        state: present
      notify: restart isc-dhcp-server

    # 4. Ensure the DHCP service is running and enabled at boot
    - name: Ensure DHCP service is active and enabled
      ansible.builtin.systemd_service:
        name: isc-dhcp-server
        state: restarted # Using 'restarted' ensures a fresh start; consider 'started' for idempotence.
        enabled: yes

    ### ──────────────────────────────────────────────────────────────────
    ###                               DNS
    ### ──────────────────────────────────────────────────────────────────

    # 5. Install the BIND9 DNS server package
    # BIND9 provides DNS resolution services for the local network (e.g., for the .devops domain).
    - name: Install BIND9 DNS server
      ansible.builtin.apt:
        name: bind9
        state: latest

    # 6. Deploy BIND9 global options configuration
    # Sets general DNS server parameters like listening interfaces, forwarders, and ACLs.
    - name: Deploy BIND9 options configuration
      ansible.builtin.copy:
        src: ../files/dns/named.conf.options
        dest: /etc/bind/named.conf.options
        owner: root
        group: bind
        mode: "0644"
      notify: restart bind9

    # 7. Deploy BIND9 local zones configuration
    # Defines the DNS zones (domains) for which this server is authoritative (e.g., bruno.icaro.devops).
    - name: Deploy BIND9 local zones configuration
      ansible.builtin.copy:
        src: ../files/dns/named.conf.internal-zones
        dest: /etc/bind/named.conf.local
        owner: root
        group: bind
        mode: "0644"
      notify: restart bind9

    # 8. Deploy forward lookup zone file
    # Contains the A records mapping hostnames to IP addresses within the primary domain.
    - name: Deploy forward lookup zone file
      ansible.builtin.copy:
        src: ../files/dns/bruno.icaro.devops.db
        dest: /etc/bind/bruno.icaro.devops.db
        owner: root
        group: bind
        mode: "0644"
      notify: restart bind9

    # 9. Deploy reverse lookup zone file
    # Contains the PTR records mapping IP addresses back to hostnames for the 192.168.56.0/24 subnet.
    - name: Deploy reverse lookup zone file
      ansible.builtin.copy:
        src: ../files/dns/56.168.192.db
        dest: /etc/bind/56.168.192.db
        owner: root
        group: bind
        mode: "0644"
      notify: restart bind9

    # 10. Ensure the DNS service is running and enabled at boot
    - name: Ensure DNS service is active and enabled
      ansible.builtin.systemd_service:
        name: bind9
        state: started
        enabled: yes

    # 11. Configure the machine to use itself as DNS server
    - name: Configure local DNS resolution
      ansible.builtin.copy:
        dest: /etc/resolv.conf
        content: |
          nameserver 127.0.0.1
          nameserver 192.168.56.128
          domain bruno.icaro.devops
          search bruno.icaro.devops
        owner: root
        group: root
        mode: "0644"

    ### ──────────────────────────────────────────────────────────────────
    ###                               LVM
    ### ──────────────────────────────────────────────────────────────────

    # 12. Install LVM2 and partitioning tools
    # Installs the necessary packages to manage Logical Volume Manager (LVM) and disk partitions.
    - name: Install LVM and partitioning tools
      ansible.builtin.apt:
        name:
          - lvm2
          - parted
        state: lastest

    # 13. Create primary partitions on each additional disk
    # Uses parted to create a single primary partition (number 1) on each of the three extra disks.
    - name: Create primary partitions on disks
      community.general.parted:
        device: "/dev/{{ item }}"
        number: 1
        state: present
        part_type: primary
        fs_type: ext4
        resize: yes
      loop: "{{ partitions }}"

    # 14. Initialize partitions as LVM Physical Volumes (PVs)
    # Prepares each partition for use with LVM by creating a Physical Volume on it.
    # The 'creates' argument makes the task idempotent; it will skip if the PV already exists.
    - name: Create LVM Physical Volumes
      ansible.builtin.command: pvcreate /dev/{{ item }}1
      loop: "{{ partitions }}"
      args:
        creates: "/dev/{{ item }}1"

    # 15. Create an LVM Volume Group (VG) named 'dados'
    # Combines the three Physical Volumes into a single storage pool.
    - name: Create LVM Volume Group
      ansible.builtin.command: vgcreate {{ vg_name }} /dev/sdb1 /dev/sdc1 /dev/sdd1
      args:
        creates: "/dev/{{ vg_name }}"

    # 16. Create an LVM Logical Volume (LV) named 'ifpb'
    # Allocates 15GB of space from the 'dados' Volume Group to create a usable block device.
    - name: Create LVM Logical Volume
      ansible.builtin.command: lvcreate -L {{ lv_size }} -n {{ lv_name }} {{ vg_name }}
      args:
        creates: "/dev/{{ vg_name }}/{{ lv_name }}"

    # 17. Format the Logical Volume with the ext4 filesystem
    - name: Format Logical Volume as ext4
      community.general.filesystem:
        fstype: ext4
        dev: "/dev/{{ vg_name }}/{{ lv_name }}"

    # 18. Create the final mount point directory
    - name: Create mount point directory
      ansible.builtin.file:
        path: "{{ mount_point }}"
        state: directory

    # 19. Mount the Logical Volume and add entry to /etc/fstab
    # Permanently mounts the LV at /dados with default options, ensuring it mounts at boot.
    - name: Mount and persist Logical Volume
      ansible.posix.mount:
        path: "{{ mount_point }}"
        src: "/dev/{{ vg_name }}/{{ lv_name }}"
        fstype: ext4
        state: mounted
        opts: defaults

    ### ──────────────────────────────────────────────────────────────────
    ###                           NFS & AutoFS
    ### ──────────────────────────────────────────────────────────────────

    # 20. Install the NFS kernel server package
    - name: Install NFS kernel server
      ansible.builtin.apt:
        name: nfs-kernel-server
        state: present

    # 21. Create the dedicated NFS system user
    # This user's UID/GID will be used for the 'all_squash' mapping. It has no login shell for security.
    - name: Create dedicated NFS system user
      ansible.builtin.user:
        name: "{{ nfs_user }}"
        shell: /usr/sbin/nologin
        create_home: no
        system: yes
        state: present

    # 22. Create and configure the directory to be shared via NFS
    # Sets ownership and permissions. Using 0755 ensures the NFS user has full access and others can read/enter.
    - name: Create and configure NFS shared directory
      ansible.builtin.file:
        path: "{{ shared_dir }}"
        state: directory
        owner: "{{ nfs_user }}"
        group: "{{ nfs_user }}"
        mode: "0755" # Author's note: CHANGING TO 0755 SOLVES PROBLEM

    # 23. Retrieve the UID of the NFS system user
    # Stores the UID in a variable for use in the NFS export configuration.
    - name: Get NFS user UID
      ansible.builtin.command: id -u {{ nfs_user }}
      register: uid
      changed_when: false

    # 24. Retrieve the GID of the NFS system user's primary group
    - name: Get NFS user GID
      ansible.builtin.command: id -g {{ nfs_user }}
      register: gid
      changed_when: false

    # 25. Store the UID and GID as Ansible facts
    # Makes the dynamically retrieved IDs available for templating in the next task.
    - name: Store NFS UID and GID as facts
      ansible.builtin.set_fact:
        nfs_uid: "{{ uid.stdout }}"
        nfs_gid: "{{ gid.stdout }}"

    # 26. Configure the NFS export in /etc/exports
    # Defines the directory to share, the allowed network, and critical options:
    # rw=read-write, sync=synchronous writes, all_squash=map all remote users,
    # anonuid/anongid=map to the specified local UID/GID (nfs-ifpb).
    - name: Configure NFS export
      ansible.builtin.lineinfile:
        path: /etc/exports
        line: "{{ shared_dir }} {{ subnet }}(rw,sync,no_subtree_check,all_squash,anonuid={{ nfs_uid }},anongid={{ nfs_gid }})"
        insertafter: EOF
        create: yes # Creates the /etc/exports file if it doesn't exist
      notify: restart nfs-kernel-server

    # 27. Ensure the NFS service is running and enabled at boot
    - name: Ensure NFS service is active and enabled
      ansible.builtin.systemd_service:
        name: nfs-kernel-server
        state: started
        enabled: yes

  handlers:
    # Handler to restart the DHCP server if its configuration changed
    - name: restart isc-dhcp-server
      ansible.builtin.systemd_service:
        name: isc-dhcp-server
        state: restarted

    # Handler to restart the DNS server if its configuration changed
    - name: restart bind9
      ansible.builtin.systemd_service:
        name: bind9
        state: restarted

    # Handler to restart the NFS server if its configuration changed
    - name: restart nfs-kernel-server
      ansible.builtin.systemd_service:
        name: nfs-kernel-server
        state: restarted
